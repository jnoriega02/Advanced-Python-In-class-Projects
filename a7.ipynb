{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "The goal of this assignment is to work with the file system, concurrency, and basic data processing in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Name & Z-ID (5 pts)\n",
    "\n",
    "Juviny Noriega\n",
    "\n",
    "Z1917876\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download & Extract Files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b. [CSCI 503] Download & Extract Files (30 pts)\n",
    "\n",
    "490 student attempting extra credit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import aiohttp,asyncio,os\n",
    "\n",
    "#function that sucessfully fetches files and downloads\n",
    "\n",
    "\n",
    "#function to read one file at a time\n",
    "async def download_file(session, base_url, filename, directory='downloads'):\n",
    "    if not os.path.exists(directory): \n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):  # Check if file has already been downloaded\n",
    "        print(f\"Starting download from {base_url}{filename}\")\n",
    "        async with session.get(base_url + filename) as response:\n",
    "            if response.status == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    while True:\n",
    "                        chunk = await response.content.read(1024)\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        f.write(chunk)\n",
    "                print(f\"Downloaded {filename} successfully\")\n",
    "            else:\n",
    "                print(f\"Failed to download {filename}. Status code: {response.status}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists.\")\n",
    "\n",
    "# Function to handle multiple downloads\n",
    "async def download_all_files(base_url, file_names):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [asyncio.create_task(download_file(session, base_url, filename)) for filename in file_names]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab.zip already exists.\n",
      "c.zip already exists.\n",
      "df.zip already exists.\n",
      "gk.zip already exists.\n",
      "ln.zip already exists.\n",
      "o.zip already exists.\n",
      "pr.zip already exists.\n",
      "s.zip already exists.\n",
      "tv.zip already exists.\n",
      "wz.zip already exists.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Directly await the async function in the notebook sinc ejupter notebook does asyncio.run()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m download_all_files(base_url, file_names)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "base_url = \"https://faculty.cs.niu.edu/~dakoop/cs503-2024sp/a7/\"\n",
    "file_names = [\"ab.zip\", \"c.zip\", \"df.zip\", \"gk.zip\", \"ln.zip\", \"o.zip\", \"pr.zip\", \"s.zip\", \"tv.zip\", \"wz.zip\"]\n",
    "\n",
    "# Directly await the async function in the notebook sinc ejupter notebook does asyncio.run()\n",
    "results = await download_all_files(base_url, file_names)\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Matching Files (15 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' Some counties have their data stored in the numpy format (.npy) while others use the CSV format (.csv). In addition, some counties have \n",
    "# updated data that we want to use (not the original, older data). In these cases, there is a directory named mod or update in the second level \n",
    "# of the zip file (e.g. data/ab/mod) that contains the files we need. For a given letter (e.g. a.npy), if there is a file in a subdirectory of a mod \n",
    "# or update directory, use it. If there is not a file in that subdirectory, use the original file. Also ignore files with extensions that are not .npy or .csv. \n",
    "# Create a list of all the paths (as pathlib.Path objects) that will need to be processed. Note that some files may'\n",
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_zip_files(directory='downloads'):\n",
    "    for zip_filename in os.listdir(directory):\n",
    "        zip_path = Path(directory) / zip_filename\n",
    "        extract_path = zip_path.parent / zip_filename[:-4] / 'data'\n",
    "\n",
    "        if not extract_path.exists():\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "            print(f\"Extracted {zip_filename}\")\n",
    "        else:\n",
    "            print(f\"Extraction directory {extract_path} already exists\")\n",
    "\n",
    "\n",
    "def find_priority_files(directory='downloads'):\n",
    "    base_path = Path(directory)\n",
    "    priority_files = []\n",
    "\n",
    "    for zip_folder in base_path.iterdir():\n",
    "        if zip_folder.is_dir():\n",
    "            data_dir = zip_folder / 'data'\n",
    "            if data_dir.exists():\n",
    "                print(f\"Checking {data_dir}\")  # Debug print\n",
    "                for folder in data_dir.iterdir():\n",
    "                    mod_update_dirs = [d for d in folder.rglob('*') if d.is_dir() and d.name in ['mod', 'update']]\n",
    "                    found = False\n",
    "\n",
    "                    for mod_dir in mod_update_dirs:\n",
    "                        for file in mod_dir.rglob('*'):\n",
    "                            if file.suffix in ['.npy', '.csv']:\n",
    "                                priority_files.append(file)\n",
    "                                found = True\n",
    "                    \n",
    "                    if not found:\n",
    "                        for file in folder.rglob('*'):\n",
    "                            if file.suffix in ['.npy', '.csv']:\n",
    "                                priority_files.append(file)\n",
    "\n",
    "    return priority_files\n",
    "\n",
    "\n",
    "# Get the list of prioritized file paths\n",
    "file_paths = find_priority_files()\n",
    "for path in file_paths:\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structural Pattern Matching to Process a File (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_file(p: Path):\n",
    "    # Identify if the file is from a mod/update directory or not\n",
    "    updated = 'mod' in p.parts or 'update' in p.parts\n",
    "\n",
    "    match p.suffix, updated:\n",
    "        case '.npy', True:\n",
    "            df = pd.DataFrame(np.load(p, allow_pickle=True))\n",
    "        case '.csv', True:\n",
    "            df = pd.read_csv(p)\n",
    "        case '.npy', False:\n",
    "            df = pd.DataFrame(np.load(p, allow_pickle=True))\n",
    "            df['value'] *= 10  # Modify the 'value' column for non-updated npy files\n",
    "        case '.csv', False:\n",
    "            df = pd.read_csv(p)\n",
    "            df['value'] *= 10  # Modify the 'value' column for non-updated csv files\n",
    "    \n",
    "    # Replace -999.0 with NaN in 'value' and 'number_of_accounts'\n",
    "    df.replace(-999.0, np.nan, inplace=True)\n",
    "    \n",
    "    # Filter records where 'data_class' is 'electricity'\n",
    "    df_electricity = df[df['data_class'] == 'electricity']\n",
    "    \n",
    "    return df_electricity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using Threading to Process All Files (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab.zip already exists.\n",
      "c.zip already exists.\n",
      "df.zip already exists.\n",
      "gk.zip already exists.\n",
      "ln.zip already exists.\n",
      "o.zip already exists.\n",
      "pr.zip already exists.\n",
      "s.zip already exists.\n",
      "tv.zip already exists.\n",
      "wz.zip already exists.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[0;32m     21\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m---> 22\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jerem\\anaconda3_2\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\jerem\\anaconda3_2\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[40], line 15\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m file_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mab.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgk.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mln.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpr.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtv.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwz.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m download_all_files(base_url, file_names)\n\u001b[1;32m---> 15\u001b[0m concatenated_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m concatenated_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     17\u001b[0m     write_yearly_files(concatenated_df, \u001b[38;5;241m2021\u001b[39m, \u001b[38;5;241m2023\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m, in \u001b[0;36mprocess_all_files\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_all_files\u001b[39m(file_paths):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m----> 3\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(process_file, (Path(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m file_paths \u001b[38;5;28;01mif\u001b[39;00m path)))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[0;32m      5\u001b[0m         concatenated_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "def process_all_files(file_paths):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_file, (Path(path) for path in file_paths if path)))\n",
    "    if results:\n",
    "        concatenated_df = pd.concat(results, ignore_index=True)\n",
    "        return concatenated_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no results\n",
    "\n",
    "# Main routine to orchestrate tasks\n",
    "async def main():\n",
    "    base_url = \"https://example.com/path/to/files/\"\n",
    "    file_names = [\"ab.zip\", \"c.zip\", \"df.zip\", \"gk.zip\", \"ln.zip\", \"o.zip\", \"pr.zip\", \"s.zip\", \"tv.zip\", \"wz.zip\"]\n",
    "    file_paths = await download_all_files(base_url, file_names)\n",
    "    concatenated_df = process_all_files(file_paths)\n",
    "    if not concatenated_df.empty:\n",
    "        write_yearly_files(concatenated_df, 2021, 2023)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
